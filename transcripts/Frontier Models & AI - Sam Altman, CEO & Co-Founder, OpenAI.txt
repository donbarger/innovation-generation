Title: Frontier Models & AI | Sam Altman, CEO & Co-Founder, OpenAI
Video ID: FytajtbCY94

Let me start with some good news. I don't know if you know this, but we are the first design partner for Codex. I did know that. Thank you. And thank you. And in the past few months, I think we've hit an exponential curve. AI Defense was a product that we launched over here last year. In about two weeks or three weeks, 100% of the code in AI defense will be written with codecs. That's unbelievable. Codex has been my biggest update on AI in a while. The app that we launched yesterday, it just pushed things over the edge for me of where it's. I'm like, all right, this is going to create an unbelievable amount of economic value extremely quickly. This is going to change how all of OpenAI works and change how other companies work it. Yeah, the models really hit some threshold and I think now the interface and harness has caught up. And I think this is the first. You're talking about chatgpt moments. I think this is like the first time I felt another chatgpt moment. For here is a clear, clear glimpse at the future of knowledge work and how enterprises and individual people are going to use AI to work in a completely different way. So what's the upper limit on this, do you think? I mean, the upper limit I think, is like full AI companies. There's probably an upper limit beyond that. The current one I can think about is full AI companies. And that seems very powerful. Like the idea that coding model can create a full, complex piece of software, but also interact with the rest of the real world to build a company around it is a very big deal. And this notion of what's happening right now, I'm sure you're tracking this with multiple. And what happened with cloudbot, is it just a passing fad, or do you think that there's something that we should take away from that on? No, I think it is definitely not a passing. Well, Multbook may be, I don't know, but openclaw is not. I think this idea that code is really powerful, but code plus generalized computer use is even much more powerful is here to stay. The fact. When I initially installed codecs, I said I was never going to give Codex full control of my computer without checking what it was doing. And that lasted about two hours because it was so useful. And then I kind of got persuaded by people that I really shouldn't have it running like that. So now I have two laptops until I figure out how this is all going to work. But giving an AI agent full use of your computer and your web Browser with all your sessions leads to incredible stuff and that seems here to stay. The idea that we can take a codec style workflow and expand it to all knowledge work so that whatever you're doing codecs can be or some version of that can be using your computer and using the web for you and editing documents and whatever other kind of work you need feels like a genuine transformation in how knowledge work will happen. And that I think openclaw did an incredible job of bringing many ideas together to make that feel usable and real. And that seems certain to be part of our future. I think Multbook is cool and I think it points to something that will be real. And maybe it will be Multbook. But there will be new kinds of social interaction where you have many agents in a space interacting with each other on behalf of people, leading to all sorts of new things. You can imagine a totally new kind of social network where everybody makes an agent or many agents and puts them in there and the agents are talking and doing stuff and finding them people and information and collaborating with other people's agents to come up with new ideas that I'm sure there will be an interesting. I think the future of social may look something like that. Very different than today. Whether it's multiple or not, uncertain. Sure. So if you think about the constraints that we are facing today, besides the obvious ones, the infrastructure and the compute and the power and all of those pieces, are there any non obvious constraints that are actually holding us back? Because people always say in the short term you always overestimate the impact of these technologies. In the longer term it's going to be grossly underestimated. But what are the non obvious constraints that you see right now? Like you're like, man, I wish I had a magic wand if I change that. So I think the obvious constraints are the biggest ones still. Energy manufacturing, enough hardware, all that kind of stuff. The non obvious ones that are most top of mind for me. One, how are we going to balance the security and data access versus the utility of all of these models? I don't think anyone has a great answer to this yet. It feels to me like there is a new kind of security or data access paradigm that needs to be invented for this. Another is how are we going to rewrite all software to be equally usable by human humans and AIs? There's like a bunch of weird quirks right now about trying to do that with the way software works. Does that change the architecture of the software itself where you're going to optimize it for agents more so than humans. And so it fundamentally changes how you build software. Yes, and there's like big examples of that and then there's like dumb examples of that. So for example, I would love my agent to be able to use Slack on my behalf because I hate drowning in Slack and I think it's like this chaotic mess for me. But it's important. But the way it works right now is my agent can use the Slack web interface and go read all my threads and do something for me, but then it has marked a bunch of stuff as read in the process of doing that and it's broken my workflows. Broken your workflows? Yeah. So I don't like. That's just a silly example of how software is not like a lot of software is not quite meant for an AI and a person to be using it together. Maybe we'll want AIs to have different sort of user accounts in some ways using the same kinds of things. Maybe a lot of software will get rewritten so that it's primarily or largely used by AI, but also still works for people using it the old fashioned way. Another kind of non obvious block is how like one of the most powerful things about AI is you can do this sort of always on computing where you could have an AI listening to your meeting or watching your meeting and watching what you're doing on your computer and then just like add a lot of value and do stuff for you. We don't even our existing computer hardware is not really meant for that. Our permissioning system and how we think about what an AI gets to see and do stuff with and what it gets to keep is not really meant for that. Our legal system doesn't really support that. Well, you'd like to be able to record a meeting and learn something from it and delete the recording. So I think there's a lot of just usability, things like that. And then you talk about this notion of. One of the things that I find is a huge dichotomy is there's so much advancement that's happening in science and in, in all the different areas that we'll talk about. Kevin Weil's going to be here later today. On the other hand, the kind of challenges that organizations struggle with to just get the basics up and running with everyone, there's a capabilities overhang where you've got. And by the way, this is a new concept. When Microsoft Word came out, people used 2% of the capability back then. And so when you start thinking about this. What? How do we make sure that we actually increase the absorption rate? You've got CIOs and CISOs over here. What advice would you give them on increasing the absorption rate within their organization for AI? Yeah, you said a bunch of interesting stuff there. The capability overhang feels to me like the biggest it has ever felt. I used to say like a few months ago, maybe I would have said it feels bigger than any time, except right when ChatGPT, right before ChatGPT launched, it now feels even bigger than that. Even though people are using AI for a lot of stuff, the fact that AI can make small but increasingly big scientific discoveries, the fact that AI can write full pieces of software, the fact that soon AI can do more generalized knowledge work, those are all huge, huge things. We have always said that we're going to automate research, use that to be able to automate the economy, use that to be able to deliver incredible new value to people and share all these benefits and this whole new technological world. And we always said that as a sort of like abstract sometime in the future thing. And yet now AI is doing research and AI is able to do huge amounts of economic work. And this, as you said, has happened many times in the past, but living through it definitely feels like, huh, this is not quite how it seems like it should go. The diffusion, the absorption is so slow. Is it slower than you thought it would be? Yes, but I think I was just naive and didn't think about it that hard. And in retrospect and looking at the history, it shouldn't be surprising. It feels fast in some ways relative to other things like ChatGPT grew crazily quickly relative to any other piece of software before. Codex is growing extremely quickly. I expect the general purpose computer use knowledge work to grow quickly. And yet looking at what's capable, looking at what's possible, it does feel sort of surprisingly slow in terms of advice. Figuring out how to set up enterprises such that they can quickly absorb these new tools and not have a year or multiple years of how are we going to think about the sort of data access and security questions that are currently blocking people, even at the level today of adopting things like codecs, that feels very important. And I don't want to make too dramatic of a prediction, but I think that companies that are not set up to be able to adopt, let's call them AI coworkers very quickly will be at a hugest advantage. And it is going to take a lot of work and some risk to be able to adopt those One of the big insights we had when we were using Codex was for the first two or three months, we kept thinking it was gonna be this kind of amazing tool. And then there was this light bulb that went off with actually one of your forward deployed engineers, and they said, you folks are thinking about this wrong because you have to think about it like a teammate rather than a tool. And that I think that level of skeuomorphic design, I don't think people have fully groked that when it comes to this because it's still a very transactional tool in most people's minds. The Codex app is the first time to me, it has truly felt like interacting with a teammate. And I think one of the lessons there is, even if you have amazing technology, which the models got, I think 5.2 was the model that got super good. But even once you get there, there's still so much value in how you package it, how you, how you have users interact with it, how easy you can make it. But this, it really does to me now feel in a way that talking to ChatGPT has always clearly felt like a tool that I am like working with a collaborator now. Right, right. And that seems very much like the shape of things to come. Okay, so let's talk about. There's a couple of topics I want to make sure I hit with you. On infrastructure. You've actually spent an enormous amount of time, even on the power side of the house. Talk to us about what happens with in general, the constraints that we have around power, constraints we have on infrastructure. How are you thinking about it? You're clearly putting enough money behind it, I guess so. All the evidence we see right now is that AI models will continue to get much more capable and also that they will get much less expensive and use many fewer resources per task. And all of the history we've seen of every time that happens is people want to use them much, much more. So we are planning for a world where AI usage grows at an accelerated pace each year. Do you think people are underestimating the capacity that's going to be needed even now, after everything that's being built out right now, they're saying like $5 trillion will be spent on this over the course of the next. Maybe if that really gets spent quickly, that will be enough. And I think it's also possible that along the way we have some supply gluts temporarily. But over a period of decades, it seems certain to me that the world is going to need a lot more tokens and will make each token Way more efficient. You'll all have a device in your pocket running super powerful models off of a battery. But there will still be this drive for more and more and more. I think the world has come to realize this and capitalism is doing its thing and supply chains are reconfiguring, policy is changing and we are going to build an incredible amount of infrastructure now. Is it enough? People always talk about the total market demand for AI. It feels to me something like the market demand for electricity or energy. You can't, you can't talk about that as a general thing. You can say how much demand there will be at different price levels and in this case you can say at different price levels for different quality, like how smart it is or how fast it is or things like that. But if we continue to make AI really capable and really cheap, there will be a ton of demand at some price. If it's more expensive, there will be less demand. But I would like the world to just get to use a ton of it. We're in this capability already now, as we mentioned, where people are like, oh, I can use it for chatter, maybe some people understand you can use it for code. I think this will just be like how we do stuff, how companies run, how scientific discovery happens, how we use most software personally in our lives and making a lot of it, if we can have it at a reasonable price, seems like a very good bet. Do you worry about the fact that the US has not had as much of a lead on the open source side? Talk to us a little bit about that. I do worry about that, Yeah. I think we should do more there, yes. What's stopping you from doing it? Focus and time. But I think we need to solve that somehow. Just play this out in your simulator for a second. If we don't have a substantial open source presence, and of course China does, versus if we do, how does the world shape differently? To be clear, I think it's most important that we lead on frontier models and I expect those to be accessed via APIs and other products. So it would be okay, but not great if we didn't also have the open source lead. People want their own models, people want control of their own models, people want to run models locally. Especially if you think about a world where you have a model that is going to see your whole life, you're going to have a new kind of device that's sort of always on and kind of keeping track of everything and adds huge value to you. I at least would really like that. Running on inference Eye control. And so I think people are going to need this. You know, again, if we get behind, obviously it's not the end of the world, but I would really like us to lead there. And I think there will be increasing demand for locally running private models. And if you were to fast forward back and start thinking, because one of the things that has been, I don't even know how you folks have done it within the compression of time that you've done it in, because there must have been moments where you decided that we're going to go full stack and we're going to now start building our own inference chips and we're going to start making sure that we have data center build outs that are happening. And it has to have been a conscious decision when you started actually going beyond the core that you had initially started with. Talk about the business model that you have today and how does that evolve and change over time? I mean the obvious ones are, of course advertising is a huge unlock when that happens. But talk about the business model in general. Are you happy with the conversion rates of free to paid? Are you happy with the take rate that people are using this system with? And then what would you like to see more of? So we have kind of two large products and now we have some others that are coming up. We have ChatGPT and our API business and now we have Codex, which looks like it's going to get very big and a few other things. And in the future we'll have consumer devices and robots and all sorts of other parts of this too. People do seem very willing to pay for AI as a subscription service. Not everybody, but a lot of people are more than we thought would. Businesses of course are willing to pay for things like ChatGPT Enterprise, but even consumers, you've reconditioned the consumers quite a bit. Consumers. I mean, I don't know the exact number off the top of my head, but many, many tens of millions of consumers paying a subscription fee. And that was a surprise to me, to be honest, a happy one. So that I think can go further. And as we add things like Codex, people are willing to pay much more there. Advertising for mass scale consumer businesses does seem like a good model, although I think we'll have to be very careful in how we do that. And then businesses increasingly want something like an AI cloud subscription where they're like, I want to partner with an AI company. I want you to handle security and context linking and access and I want to be able to run lots of agents on it. I want A general purpose platform there. I want some agents from you, some agents from other people. I may even want to run other people's models. I want a ChatGPT enterprise license, I want a ton of API access, things like that. So I think there will be a model there as well. If you were to think of you've had an interesting model that you've talked about, which is participating in the upside on scientific discovery as well. If you think about the most mature ones, clearly subscription is very mature, clearly advertising is very mature. At this point in time, it's quite clear that you're going to participate in both of those and then you'll have other ones that keep coming out. Yeah, to be very clear, we don't want to take a share of people that are just paying us for the API and making discoveries. That's great. And that's theirs. I can imagine though, that if there is a future where we can spend billions of dollars on inference and cure an important disease, we may explore partnerships there where we pay for that cost in partnership with the drug company and then kind of get some royalty on it. This is not something we're doing now, but I think the frontier of scientific discovery with AI will require so much capital that maybe we think of ourselves as an investor in some of those cases and is our imagination of the things that can be done. Do you find yourself struggling with how far exponentially you can think about what's going on as well? You're a pretty big thinker, but it seems like there's new ideas emerging all the time. Where do you think the imagination shortage is right now with the way that these technologies are evolving? I can imagine billions of humanoid robots building more data centers and mining for material and building more power plants and all of that. I can imagine just the economy growing at an unprecedented rate as there's all sorts of incredible new services and scientific discoveries happening. I can imagine the von Neumann probes launching and then beyond that, I got no idea. That seems like a good start. Macro Headwinds, tailwinds. What do you feel like are things that you really worry about capitalizing on that are tailwinds? Because you have a window within which if you don't, then you can actually lose that tailwind. And then what are the headwinds that you worry about? The tailwind is the model. We already have this capability overhang and the models are going to get so much better quickly. We've been trying to figure out how we communicate about what we think is happening. That does not seem like hype or crazy, but the models are just going to get very, very good this year. And that is a tailwind that I think we can build. Incredible things with headwind. Yeah. Probably some sort of like global destabilization, mega supply chain disruption, something like that. And on the tailwind side, on the model's getting infinitely better. Do you think in 26 we see a 10x improvement, 100x improvement, a 5x improvement? What do you mean by. What is a 10x improvement? Things that you can. Problems you can solve that you were. Yeah, I would bet it subjectively feels something like 10x by the end of this year. I don't know how to put an exact metric on that, but that feels reasonable. Any question that I didn't ask that I should have asked, that was a lot. I feel like we covered a lot of ground. You're going to come back again? Come back here again? Yeah. I would love. If you invite me, I will. Sam Altman. Thank you. Thanks so much. Thank you.